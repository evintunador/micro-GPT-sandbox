{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8178ba-2ea0-4f4f-b9ca-c3435a83bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918a651-5c6a-4a39-8b3e-a28259e4fd64",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c0ba50-83de-4ad7-b262-944e6d547ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4011.456 K parameters \n",
      " ModelConfig(dim=192, device='cpu', tokenizer='bpe_v1', vocab_len=8192, num_layers=6, second_resid_norm=False, mlp_hidden_mult=4, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=2, num_kv_heads=1, head_dim=96, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (token_embedder): Embedding(8195, 192)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x Layer(\n",
       "      (pre_attn_norm): Norm()\n",
       "      (attn): MQA(\n",
       "        (Wq): Linear(in_features=192, out_features=192, bias=False)\n",
       "        (Wk): Linear(in_features=192, out_features=96, bias=False)\n",
       "        (Wv): Linear(in_features=192, out_features=96, bias=False)\n",
       "        (Wo): Linear(in_features=192, out_features=192, bias=False)\n",
       "      )\n",
       "      (pre_mlp_norm): Norm()\n",
       "      (mlp): MLP(\n",
       "        (Wup): Linear(in_features=192, out_features=512, bias=False)\n",
       "        (Wgate): Linear(in_features=192, out_features=512, bias=False)\n",
       "        (Wdown): Linear(in_features=512, out_features=192, bias=False)\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): Norm()\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### pretrained model options:\n",
    "## templateGPT\n",
    "# a 1m parameter model trained for 2000 iters with many layers, thin MLP hidden dimensions & few attention heads: 'models/templateGPT/trained/templateGPT_1m_tall_and_skinny'\n",
    "# a 1m parameter model trained for 2000 iters with layers, MLP hidden dimensions, & attention heads bw the other two: 'models/templateGPT/trained/templateGPT_1m_5ft11_and_skinnyfat'\n",
    "# a 1m parameter model trained for 2000 iters with few layers, thick MLP hidden dimensions & many attention heads: 'models/templateGPT/trained/templateGPT_1m_short_and_thicc'\n",
    "# a 2m parameter model trained for 4000 iters with CosineNorm: 'models/templateGPT/trained/templateGPT_2m_CosineNorm'\n",
    "# a 2m parameter model trained for 4000 iters with LayerNorm: 'models/templateGPT/trained/templateGPT_2m_LayerNorm'\n",
    "# a 2m parameter model trained for 4000 iters with RMSNorm: 'models/templateGPT/trained/templateGPT_2m_RMSNorm'\n",
    "# a 3m parameter model trained for 5000 iters with a gated MLP: 'models/templateGPT/trained/templateGPT_3m_GatedMLP'\n",
    "# a 3m parameter model trained for 5000 iters with an old-fashioned MLP: 'models/templateGPT/trained/templateGPT_3m_GatedMLP'\n",
    "# a 4m parameter model trained for 6000 iters with GeGLU activation: 'models/templateGPT/trained/templateGPT_4m_GeGLU'\n",
    "# a 4m parameter model trained for 6000 iters with SwiGLU activation: 'models/templateGPT/trained/templateGPT_4m_SwiGLU'\n",
    "## fractal-head-attention\n",
    "# a 1m parameter model designed to be compared against the equivalent templateGPT: 'models/fractal-head-attention/trained/FHA_1m_short_and_thicc'\n",
    "## Memory Mosaics\n",
    "# a 1m parameter model designed to stick close to the paper's parameter ratios: 'models/MemoryMosaics/trained/MM_1m'\n",
    "name = 'models/templateGPT/trained/templateGPT_4m_SwiGLU'\n",
    "\n",
    "from tools import load_model\n",
    "model, tokenizer, cfg = load_model(name)\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c971fce-8b3e-4732-bd66-d5d2028025d6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a366b1fc-b620-45a0-8b42-71bdca18906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing this model's specific generate function\n",
    "from tools import import_from_nested_path\n",
    "path_parts = name.split('/')\n",
    "imported_objects = import_from_nested_path([path_parts[0], path_parts[1]], 'inference', ['generate'])\n",
    "generate = imported_objects.get('generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c202ea0-e64d-4367-a4a6-102756fe63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once| |upon| |a| |ti|me|, |th|er|e| |was| |a| |boy| |na|me|d| |Tim|. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time, there was a boy named Tim. \"\n",
    "print(tokenizer.display(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b13a76-50f8-48b6-b7cf-9097d307c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum attention matrix size in memory will be 64x512 rather than 512x512\n",
      "\n",
      "Once upon a time, there was a boy named Tim. Tim loved to play with his toys and eat dirt. One day, Tim went to the park to play. He saw a big slide and wanted to climb it. Tim thought it would be fun to play with the top of the slide.\n",
      "Tim had an idea. He would climb the slide and get it back to his room. He was very happy. Tim started to climb the slide and climb it. He climbed up the slide and got his new cake. But then, something unexpected happened. The slide started to blow down the slide! Tim and his friend were surprised and happy. The sun was not safe, and Tim was not happy again. He learned that he was not happy and always trying to try new things that can help us find the cake together.\n"
     ]
    }
   ],
   "source": [
    "output = generate(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer,\n",
    "    #max_gen_len = 100,\n",
    "    temperature = 0.7,\n",
    "    memory_saver_div = 8,\n",
    "    top_p = 0.9,\n",
    "    top_k = 32,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114de8bf-ab76-460d-8c37-ebe368b47e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
