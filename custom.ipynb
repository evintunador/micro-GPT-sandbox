{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5539550a-7e7b-4822-9142-fc25d6ca667b",
   "metadata": {},
   "source": [
    "# Next-Concept Prediction v11.1\n",
    "\n",
    "So the idea here is to\n",
    "1. turn minGemma into a matryoshkaGPT (or maybe just a MatFormer, aka ignoring the attention mechanism, for simplicity?)\n",
    "2. have the smallest level deal with tokens, medium with first-level concepts, and large with 2nd-level concepts\n",
    "    - figure out the ideal token vs concept layout for the sequences\n",
    "4. make a way to dynamically generate concept vectors based on cosine similarity rather than storing huge vectors\n",
    "5. can i figure out a way to have tokens dyanmically use any given level rather than being stuck to a preset number of combinations? idk prolly not. Maybe a single sequence & an MoE router that decides which level to output at a given moment and then automatically concatenates any old tokens into the largest possible size?!?!?!?! idk how that'd work but maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# imports for the tokenizer\n",
    "from tiny_stories_tokenizer import *\n",
    "\n",
    "# Imports used for the config\n",
    "import dataclasses \n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "\n",
    "# used in the training loop\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd373bf-d326-4939-be18-819bae892805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokenizer = get_tokenizer(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for next-concept predictor\n",
    "    \"\"\"\n",
    "    ### boring hyperparameters ###\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    max_seq_len: int = 256\n",
    "    num_hidden_layers: int = 4\n",
    "    num_q_heads: int = 4\n",
    "    num_kv_heads: int = 1 \n",
    "    assert num_q_heads % num_kv_heads == 0\n",
    "    embed_dim: int = 128 \n",
    "    mlp_multiplier: int = 4\n",
    "    head_dim: int = 32\n",
    "    theta = 100.0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6384a84e-ca6c-4479-ab8c-4daf3d724b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoPE(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "    \n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "181d512a-8fa4-4e54-a511-19e07a9fd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQA(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_q_heads = config.num_q_heads\n",
    "        self.num_kv_heads = config.num_kv_heads\n",
    "        assert self.num_q_heads % self.num_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.num_q_heads // self.num_kv_heads\n",
    "\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.head_dim = config.head_dim\n",
    "        self.theta = config.theta\n",
    "\n",
    "        # Calculates the total size for all projections.\n",
    "        self.q_size = self.num_q_heads * self.head_dim\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "\n",
    "        # Defines the scaling factor for the attention scores.\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.embed_dim, (self.num_q_heads + 2 * self.num_kv_heads) * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_q_heads * self.head_dim, self.embed_dim, bias=False)\n",
    "    \n",
    "        # for our attention mask we'll create a boolean mask that'll later be turned into large negative values\n",
    "        self.mask = torch.tril(torch.ones((config.max_seq_len, config.max_seq_len), dtype=torch.uint8)\n",
    "                              ).view(1, 1, config.max_seq_len, config.max_seq_len).to(dtype=torch.bool)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len, _ = x.shape\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # Splits the combined QKV tensor into separate tensors for queries (xq), keys (xk), and values (xv) based on their respective sizes.\n",
    "        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size],dim=-1)\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.num_q_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = RoPE(xq, self.head_dim, self.theta)\n",
    "        xk = RoPE(xk, self.head_dim, self.theta)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_q_heads:\n",
    "            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2) # [batch_size, input_len, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        q = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        k = xk.transpose(1, 2)\n",
    "        v = xv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        logits = torch.matmul(q, k.transpose(2, 3)) * self.scaling # [batch_size, n_local_heads, input_len, input_len]\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention logits\n",
    "        logits = torch.where(self.mask[..., :input_len, :input_len].expand_as(logits), \n",
    "                             logits, \n",
    "                             torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = torch.matmul(scores, v) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1) # [batch_size, input_len, hidden_dim]\n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3111446c-52ff-4c36-b441-e6724d6ad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gate_proj = nn.Linear(embed_dim, hidden_size)\n",
    "        self.up_proj = nn.Linear(embed_dim, hidden_size)\n",
    "        self.down_proj = nn.Linear(hidden_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.up_proj(x) * F.gelu(self.gate_proj(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "937ec822-e4fb-4b76-9ada-37dd1f94acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, use_scale=True):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(num_features)) if use_scale else None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the mean squared value for each feature\n",
    "        mean_squared = inputs.pow(2).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # Normalize inputs\n",
    "        normed_inputs = inputs * torch.rsqrt(mean_squared + self.eps)\n",
    "\n",
    "        # Apply scale if it exists\n",
    "        if self.scale is not None:\n",
    "            normed_inputs = normed_inputs * self.scale\n",
    "\n",
    "        return normed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd1fcf1-a1dd-4e6c-8d78-e267398304f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mqa = MQA(config)\n",
    "        self.mlp = MLP(config.embed_dim, config.embed_dim * config.mlp_multiplier)\n",
    "        \n",
    "        self.pre_mqa_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "        self.post_mqa_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "        self.pre_mlp_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "        self.post_mlp_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor ) -> torch.Tensor:\n",
    "        x = x + self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x)))\n",
    "        x = x + self.post_mlp_norm(self.mlp(self.pre_mlp_norm(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dea80b7-2d2e-4e0d-9616-6acfee9b4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customGPT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        config: Config, # the hyperparameters\n",
    "        tokenizer: tokenizer, # the tokenizer. we don't always store the tokenizer inside of the model, but it doesn't matter here\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # the attention heads need to cleanly divide up the embed_dim of the model so that we can split it all apart & combine back together\n",
    "        assert config.embed_dim % config.num_q_heads == 0\n",
    "\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.head_dim = config.head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(self.vocab_size, config.embed_dim)\n",
    "        self.scaling = config.embed_dim ** 0.5 # for normalizing the first embedding\n",
    "        \n",
    "        # Initialize a sequence of DecoderLayer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_hidden_layers))\n",
    "\n",
    "        # Initialize a normalization layer to be applied after the last decoder layer, stabilizing the output\n",
    "        self.final_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len) list of integer token ids\n",
    "        target_token_ids: torch.Tensor = None, # a shape (batch_size, input_seq_len) list of token ids to train on\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        # turn the input tokens into the first resudial state using the embedding matrix\n",
    "        x = self.embedder(input_token_ids) * self.scaling # (batch_size, input_len) & (vocab_size, embed_dim) -> (batch_size, input_len, embed_dim)\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply normalization to the output of the final decoder layer\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # grabbing the weights of the embedding matrix shape (vocab_size, hidden_dim) for use as the output layer\n",
    "        embedder_weight = self.embedder.weight\n",
    "\n",
    "        # the embedding matrix is also used as the output layer\n",
    "        logits = torch.matmul(x, embedder_weight.t()) # (batch_size, input_len, embed_dim) @ (embed_dim, vocab_size) -> (batch_size, input_len, vocab_size)\n",
    "        \n",
    "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are training\n",
    "            batch_size, input_len, vocab_size = logits.shape\n",
    "            # then we reshape our logits & targets before calculating cross-entropy loss\n",
    "            loss = self.criterion(logits.view(batch_size*input_len, vocab_size), \n",
    "                                  target_token_ids.view(batch_size*input_len))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        \"\"\"\n",
    "        # Select the last element for each sequence.\n",
    "        logits = logits[:,-1,:] # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        logits.div_(temperature) # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k. both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "        ### calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        ### calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = 100, # the model will output 100 tokens by default\n",
    "        temperature: float = 0.95, # 0.95 is pretty close to not even using temperature at all (1.0 would be no effect)\n",
    "        top_p: float = 1.0, # defaulting to 1 means we essentially don't use top-p\n",
    "        top_k: int = 65, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
    "    ) -> str: \n",
    "        \"\"\" Wrapper around sampler() that deals with manipulation of the sequence \"\"\"\n",
    "        \n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        assert len(tokens) + output_len <= self.config.max_seq_len\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len])\n",
    "            \n",
    "            next_token = self.Sampler(\n",
    "                logits = logits,\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a1f17-5a2e-4bbe-bc99-e9412d2b81de",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee75deb9-dfbc-46dd-93d7-4981369a2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d02be1-d005-4913-b8b7-630a7b97696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_seq_len+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 5): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973.44 K parameters\n",
      "customGPT(\n",
      "  (embedder): Embedding(128, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (mqa): MQA(\n",
      "        (qkv_proj): Linear(in_features=128, out_features=192, bias=False)\n",
      "        (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (gate_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (up_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (down_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (pre_mqa_norm): RMSNorm()\n",
      "      (post_mqa_norm): RMSNorm()\n",
      "      (pre_mlp_norm): RMSNorm()\n",
      "      (post_mlp_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = customGPT(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5242a7-6035-4f53-9d26-0690aea809d9",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d5b32a-8fc1-4fa2-a301-85142c442bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972.416 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "minGemma(\n",
       "  (embedder): Embedding(128, 128)\n",
       "  (model): Body(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x Layer(\n",
       "        (self_attn): Attention(\n",
       "          (qkv_proj): Linear(in_features=128, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (up_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (down_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = customGPT(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/?.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 10\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 2\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 122.3788, val loss 122.5427, time elapsed: 0.77 seconds\n",
      "step 2: train loss 118.4111, val loss 118.5147, time elapsed: 4.06 seconds\n",
      "step 4: train loss 114.2624, val loss 114.5871, time elapsed: 7.23 seconds\n",
      "step 6: train loss 110.7172, val loss 111.2204, time elapsed: 10.45 seconds\n",
      "step 8: train loss 107.8699, val loss 107.6988, time elapsed: 13.78 seconds\n",
      "step 9: train loss 106.1037, val loss 106.7131, time elapsed: 16.44 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "torch.save(model.state_dict(),\n",
    "           f'models/{model.__class__.__name__}'\n",
    "           f'-vocab_size{config.vocab_size}'\n",
    "           f'-max_seq_len{config.max_seq_len}'\n",
    "           f'-num_hidden_layers{config.num_hidden_layers}'\n",
    "           f'-num_q_heads{config.num_q_heads}'\n",
    "           f'-num_kv_heads{config.num_kv_heads}'\n",
    "           f'-embed_dim{config.embed_dim}'\n",
    "           f'-mlp_multiplier{config.mlp_multiplier}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-theta{config.theta}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30e834-56f9-4812-ab4c-03e2f14874ca",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2260c4f-40b2-4b8e-9db4-ad1717a3027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou    pppwaswaswaswaswaswaswaswaswaswaswaswaswaswasFFFFFFFFFFFFFFFFFFFFzzzzzzonononononononononononononononononononononononon\"\"\"\"\"\"\"\"8888888888mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
      "\n",
      "\n",
      "mmmmvvvvvvvvvvvvvvvvv??????????????????????????????????????\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
    "max_useable_output_len = config.max_seq_len - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len, temperature=15.0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f258187-bf1d-482c-9c96-9cf21be21daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562598fd-ae5c-4c9c-a437-344d8518711b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
